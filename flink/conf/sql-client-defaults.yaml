################################################################################
# Copyright 2019 Ververica GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
################################################################################


# This file defines the default environment for Flink's SQL Client.
# Defaults might be overwritten by a session specific environment.


#==============================================================================
# Table Sources
#==============================================================================

# Define table sources here. See the Table API & SQL documentation for details.

tables:
  - name: note_info
    type: source
    update-mode: append
    schema:
    - name: user_id
      type: BIGINT
    - name: note_id
      type: BIGINT
    - name: behavior
      type: STRING
    - name: ts
      type: TIMESTAMP
      rowtime:
        timestamps:
          type: "from-field"
          from: "event_time"
        watermarks:
          type: "periodic-bounded"
          delay: "0"
    connector:
      property-version: 1
      type: kafka
      version: universal
      topic: note-info
      startup-mode: earliest-offset
      properties:
      - key: zookeeper.connect
        value: myzookeeper:2181
      - key: bootstrap.servers
        value: mykafka:9092
      - key: group.id
        value: testGroup
        #format: csv 
        # property-version: 1
        # type: csv 
        # schema: "ROW(userId BIGINT, noteId BIGINT, behavior STRING, rowtime TIMESTAMP)"
    format:
      type: csv
      # property-version: 1
      schema: "ROW(user_id BIGINT, note_id BIGINT, behavior STRING, event_time TIMESTAMP)"

  - name: login_info
    type: source
    update-mode: append
    schema:
    - name: user_id
      type: BIGINT
    - name: behavior
      type: STRING
    - name: ts
      type: TIMESTAMP
      rowtime:
        timestamps:
          type: "from-field"
          from: "event_time"
        watermarks:
          type: "periodic-bounded"
          delay: "0"
    connector:
      property-version: 1
      type: kafka
      version: universal
      topic: login-info
      startup-mode: earliest-offset
      properties:
      - key: zookeeper.connect
        value: myzookeeper:2181
      - key: bootstrap.servers
        value: mykafka:9092
      - key: group.id
        value: testGroup
        #format: csv 
        # property-version: 1
        # type: csv 
        # schema: "ROW(userId BIGINT, noteId BIGINT, behavior STRING, rowtime TIMESTAMP)"
    format:
      type: csv
      # property-version: 1
      schema: "ROW(user_id BIGINT, behavior STRING, event_time TIMESTAMP)"

  - name: login_warning
    type: sink
    # update-mode: append
    schema:
    - name: user_id
      type: BIGINT
    - name: count 
      type: BIGINT
    - name: begin_time
      data-type: TIMESTAMP(3)
    - name: end_time
      data-type: TIMESTAMP(3)
    connector:
      type: jdbc
      url: "jdbc:mysql://mymysql:3306/note"     # required: JDBC DB url
      table: "login_warning"        # required: jdbc table name
      driver: "com.mysql.jdbc.Driver" # optional: the class name of the JDBC driver to use to connect to this URL.
                                      # If not set, it will automatically be derived from the URL.
      username: "root"                # optional: jdbc user name and password
      password: "123456"
      read: # scan options, optional, used when reading from table
        partition: # These options must all be specified if any of them is specified. In addition, partition.num must be specified. They
                   # describe how to partition the table when reading in parallel from multiple tasks. partition.column must be a numeric,
                   # date, or timestamp column from the table in question. Notice that lowerBound and upperBound are just used to decide
                   # the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.
                   # This option applies only to reading.
          column: "column_name" # optional, name of the column used for partitioning the input.
          num: 50               # optional, the number of partitions.
          lower-bound: 500      # optional, the smallest value of the first partition.
          upper-bound: 1000     # optional, the largest value of the last partition.
        fetch-size: 100         # optional, Gives the reader a hint as to the number of rows that should be fetched
                                # from the database when reading per round trip. If the value specified is zero, then
                                # the hint is ignored. The default value is zero.
      lookup: # lookup options, optional, used in temporary join
        cache:
          max-rows: 5000 # optional, max number of rows of lookup cache, over this value, the oldest rows will
                         # be eliminated. "cache.max-rows" and "cache.ttl" options must all be specified if any
                         # of them is specified. Cache is not enabled as default.
          ttl: "10s"     # optional, the max time to live for each rows in lookup cache, over this time, the oldest rows
                         # will be expired. "cache.max-rows" and "cache.ttl" options must all be specified if any of
                         # them is specified. Cache is not enabled as default.
        max-retries: 3   # optional, max retry times if lookup database failed
      write: # sink options, optional, used when writing into table
          flush:
            max-rows: 5000 # optional, flush max size (includes all append, upsert and delete records), 
                           # over this number of records, will flush data. The default value is "5000".
            interval: "2s" # optional, flush interval mills, over this time, asynchronous threads will flush data.
                           # The default value is "0s", which means no asynchronous flush thread will be scheduled. 
          max-retries: 3   # optional, max retry times if writing records to database failed.
      
      
# functions:
# - name: timeDiff
#   from: class
#   class: com.ververica.sql_training.udfs.TimeDiff
# - name: isInNYC
#   from: class
#   class: com.ververica.sql_training.udfs.IsInNYC
# - name: toAreaId
#   from: class
#   class: com.ververica.sql_training.udfs.ToAreaId
# - name: toCoords
#   from: class
#   class: com.ververica.sql_training.udfs.ToCoords

#==============================================================================
# Execution properties
#==============================================================================

# Execution properties allow for changing the behavior of a table program.

execution:
  planner: blink               # using the Blink planner
  type: streaming              # 'batch' or 'streaming' execution
  result-mode: table           # 'changelog' or 'table' presentation of results
  parallelism: 1               # parallelism of the program
  max-parallelism: 128         # maximum parallelism
  min-idle-state-retention: 0  # minimum idle state retention in ms
  max-idle-state-retention: 0  # maximum idle state retention in ms

#==============================================================================
# Deployment properties
#==============================================================================

# Deployment properties allow for describing the cluster to which table
# programs are submitted to.

deployment:
  type: standalone             # only the 'standalone' deployment is supported
  response-timeout: 5000       # general cluster communication timeout in ms
  gateway-address: ""          # (optional) address from cluster to gateway
  gateway-port: 0              # (optional) port from cluster to gateway


